{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "015a61a6-6f47-4be7-b33d-5d5411357844",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65c3b1b-755b-4633-9f80-2626fe263593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/intel/inteloneapi/pytorch/latest/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0351cea9-14da-4501-9a2a-5b82d6b61dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('redacted_training_data.csv', sep=';')\n",
    "\n",
    "# def func(s):\n",
    "#     if s == 'Y':\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "# df['is_goal'] = df['is_goal'].apply(func)\n",
    "\n",
    "# # shuffle the DataFrame rows\n",
    "# df = df.sample(frac=1)\n",
    "\n",
    "# df = df.rename(columns={'is_goal': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e0b6047-9ba5-4482-92ac-a4c8486f6718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('train_dataset_4500.csv')\n",
    "# test_df = pd.read_csv('test_dataset_208.csv')\n",
    "train_df = pd.read_csv('train_dataset_1800.csv')\n",
    "test_df = pd.read_csv('test_dataset_208_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cafe45a-380e-4545-a248-5425634b7699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(base_model, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "336a083a-4653-488e-9e0d-1daab4eb0be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "# X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "# X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca125399-8e22-4f82-8471-59d467ca4723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_X = list(train_df['sentence'])\n",
    "train_y = list(train_df['label'])\n",
    "test_X = list(test_df['sentence'])\n",
    "test_y = list(test_df['label'])\n",
    "\n",
    "train_X_tokenized = tokenizer(train_X, padding=True, truncation=True, max_length=512)\n",
    "test_X_tokenized = tokenizer(test_X, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = Dataset(train_X_tokenized, train_y)\n",
    "test_dataset = Dataset(test_X_tokenized, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ef21db4-d5f1-4bbe-89bc-8630dfdd8866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u177598/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1800\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 678\n",
      "  Number of trainable parameters = 109483778\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='678' max='678' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [678/678 1:17:53, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.695400</td>\n",
       "      <td>0.672596</td>\n",
       "      <td>0.543269</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.086538</td>\n",
       "      <td>0.159292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.645000</td>\n",
       "      <td>0.622467</td>\n",
       "      <td>0.629808</td>\n",
       "      <td>0.575419</td>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.727915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.588400</td>\n",
       "      <td>0.521643</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.522600</td>\n",
       "      <td>0.476878</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.751938</td>\n",
       "      <td>0.932692</td>\n",
       "      <td>0.832618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.459415</td>\n",
       "      <td>0.778846</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.682692</td>\n",
       "      <td>0.755319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.428300</td>\n",
       "      <td>0.445664</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.830357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.502600</td>\n",
       "      <td>0.438141</td>\n",
       "      <td>0.822115</td>\n",
       "      <td>0.759690</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.841202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.322900</td>\n",
       "      <td>0.396349</td>\n",
       "      <td>0.841346</td>\n",
       "      <td>0.838095</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.387700</td>\n",
       "      <td>0.528985</td>\n",
       "      <td>0.759615</td>\n",
       "      <td>0.684932</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.400400</td>\n",
       "      <td>0.418385</td>\n",
       "      <td>0.836538</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.821053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.462800</td>\n",
       "      <td>0.378768</td>\n",
       "      <td>0.831731</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.840183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.250700</td>\n",
       "      <td>0.393122</td>\n",
       "      <td>0.836538</td>\n",
       "      <td>0.801724</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.845455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.373800</td>\n",
       "      <td>0.382894</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.237600</td>\n",
       "      <td>0.462485</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.913462</td>\n",
       "      <td>0.829694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.251100</td>\n",
       "      <td>0.373516</td>\n",
       "      <td>0.850962</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>0.855769</td>\n",
       "      <td>0.851675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.261300</td>\n",
       "      <td>0.398071</td>\n",
       "      <td>0.836538</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.841121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.342900</td>\n",
       "      <td>0.411840</td>\n",
       "      <td>0.841346</td>\n",
       "      <td>0.803419</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.850679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.348500</td>\n",
       "      <td>0.406100</td>\n",
       "      <td>0.836538</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.841121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.834783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.186800</td>\n",
       "      <td>0.395527</td>\n",
       "      <td>0.836538</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.838095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.264000</td>\n",
       "      <td>0.397827</td>\n",
       "      <td>0.836538</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.841121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.235000</td>\n",
       "      <td>0.391600</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.849057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.322800</td>\n",
       "      <td>0.387144</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.839623</td>\n",
       "      <td>0.855769</td>\n",
       "      <td>0.847619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.145500</td>\n",
       "      <td>0.395828</td>\n",
       "      <td>0.850962</td>\n",
       "      <td>0.828829</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.855814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.135400</td>\n",
       "      <td>0.418688</td>\n",
       "      <td>0.850962</td>\n",
       "      <td>0.828829</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.855814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.115700</td>\n",
       "      <td>0.452228</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.827273</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.850467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.127300</td>\n",
       "      <td>0.529365</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.795082</td>\n",
       "      <td>0.932692</td>\n",
       "      <td>0.858407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.135500</td>\n",
       "      <td>0.476017</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.865385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.148800</td>\n",
       "      <td>0.474131</td>\n",
       "      <td>0.870192</td>\n",
       "      <td>0.859813</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.872038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.157400</td>\n",
       "      <td>0.468132</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.867925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.456564</td>\n",
       "      <td>0.855769</td>\n",
       "      <td>0.842593</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.858491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.330100</td>\n",
       "      <td>0.444589</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.870370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.112400</td>\n",
       "      <td>0.473067</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.854545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.174600</td>\n",
       "      <td>0.512934</td>\n",
       "      <td>0.836538</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.844037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>0.502604</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.843137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.096200</td>\n",
       "      <td>0.518500</td>\n",
       "      <td>0.855769</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.854369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.548377</td>\n",
       "      <td>0.855769</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.862385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.562768</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.858491</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.090600</td>\n",
       "      <td>0.577503</td>\n",
       "      <td>0.860577</td>\n",
       "      <td>0.850467</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.862559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.673489</td>\n",
       "      <td>0.831731</td>\n",
       "      <td>0.771654</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.848485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.148400</td>\n",
       "      <td>0.601659</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.855769</td>\n",
       "      <td>0.864078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.124000</td>\n",
       "      <td>0.642995</td>\n",
       "      <td>0.860577</td>\n",
       "      <td>0.871287</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.858537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.084500</td>\n",
       "      <td>0.596709</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.867925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.619075</td>\n",
       "      <td>0.841346</td>\n",
       "      <td>0.808696</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.849315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.055100</td>\n",
       "      <td>0.595438</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.865385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.632837</td>\n",
       "      <td>0.841346</td>\n",
       "      <td>0.803419</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.850679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.679435</td>\n",
       "      <td>0.841346</td>\n",
       "      <td>0.798319</td>\n",
       "      <td>0.913462</td>\n",
       "      <td>0.852018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.644998</td>\n",
       "      <td>0.860577</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.865116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.674891</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.853211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.063700</td>\n",
       "      <td>0.686771</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.853211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.072900</td>\n",
       "      <td>0.679868</td>\n",
       "      <td>0.855769</td>\n",
       "      <td>0.842593</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.858491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>0.696870</td>\n",
       "      <td>0.855769</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.851485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>0.675726</td>\n",
       "      <td>0.860577</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.861244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.730411</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.854545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.071800</td>\n",
       "      <td>0.696634</td>\n",
       "      <td>0.850962</td>\n",
       "      <td>0.823009</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>0.660488</td>\n",
       "      <td>0.860577</td>\n",
       "      <td>0.864078</td>\n",
       "      <td>0.855769</td>\n",
       "      <td>0.859903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.077600</td>\n",
       "      <td>0.670955</td>\n",
       "      <td>0.850962</td>\n",
       "      <td>0.828829</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.855814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>0.676540</td>\n",
       "      <td>0.855769</td>\n",
       "      <td>0.830357</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.861111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.664137</td>\n",
       "      <td>0.860577</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.865116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.026800</td>\n",
       "      <td>0.656801</td>\n",
       "      <td>0.860577</td>\n",
       "      <td>0.844037</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.863850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.671464</td>\n",
       "      <td>0.860577</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.865116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.665856</td>\n",
       "      <td>0.860577</td>\n",
       "      <td>0.844037</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.863850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.667420</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.845455</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.869159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.680994</td>\n",
       "      <td>0.860577</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.865116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.676023</td>\n",
       "      <td>0.860577</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.865116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.683782</td>\n",
       "      <td>0.860577</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.865116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>0.692246</td>\n",
       "      <td>0.860577</td>\n",
       "      <td>0.831858</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.866359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-10\n",
      "Configuration saved in mar9/checkpoint-10/config.json\n",
      "Model weights saved in mar9/checkpoint-10/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-10/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-10/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-20\n",
      "Configuration saved in mar9/checkpoint-20/config.json\n",
      "Model weights saved in mar9/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-20/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-30\n",
      "Configuration saved in mar9/checkpoint-30/config.json\n",
      "Model weights saved in mar9/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-30/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-40\n",
      "Configuration saved in mar9/checkpoint-40/config.json\n",
      "Model weights saved in mar9/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-40/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-50\n",
      "Configuration saved in mar9/checkpoint-50/config.json\n",
      "Model weights saved in mar9/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-50/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-60\n",
      "Configuration saved in mar9/checkpoint-60/config.json\n",
      "Model weights saved in mar9/checkpoint-60/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-60/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-60/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-70\n",
      "Configuration saved in mar9/checkpoint-70/config.json\n",
      "Model weights saved in mar9/checkpoint-70/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-70/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-70/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-80\n",
      "Configuration saved in mar9/checkpoint-80/config.json\n",
      "Model weights saved in mar9/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-80/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-90\n",
      "Configuration saved in mar9/checkpoint-90/config.json\n",
      "Model weights saved in mar9/checkpoint-90/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-90/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-90/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-100\n",
      "Configuration saved in mar9/checkpoint-100/config.json\n",
      "Model weights saved in mar9/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-100/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-110\n",
      "Configuration saved in mar9/checkpoint-110/config.json\n",
      "Model weights saved in mar9/checkpoint-110/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-110/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-110/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-120\n",
      "Configuration saved in mar9/checkpoint-120/config.json\n",
      "Model weights saved in mar9/checkpoint-120/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-120/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-120/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-130\n",
      "Configuration saved in mar9/checkpoint-130/config.json\n",
      "Model weights saved in mar9/checkpoint-130/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-130/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-130/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-140\n",
      "Configuration saved in mar9/checkpoint-140/config.json\n",
      "Model weights saved in mar9/checkpoint-140/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-140/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-140/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-150\n",
      "Configuration saved in mar9/checkpoint-150/config.json\n",
      "Model weights saved in mar9/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-150/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-160\n",
      "Configuration saved in mar9/checkpoint-160/config.json\n",
      "Model weights saved in mar9/checkpoint-160/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-160/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-160/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-170\n",
      "Configuration saved in mar9/checkpoint-170/config.json\n",
      "Model weights saved in mar9/checkpoint-170/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-170/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-170/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-180\n",
      "Configuration saved in mar9/checkpoint-180/config.json\n",
      "Model weights saved in mar9/checkpoint-180/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-180/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-180/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-190\n",
      "Configuration saved in mar9/checkpoint-190/config.json\n",
      "Model weights saved in mar9/checkpoint-190/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-190/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-190/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-200\n",
      "Configuration saved in mar9/checkpoint-200/config.json\n",
      "Model weights saved in mar9/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-200/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-210\n",
      "Configuration saved in mar9/checkpoint-210/config.json\n",
      "Model weights saved in mar9/checkpoint-210/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-210/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-210/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-220\n",
      "Configuration saved in mar9/checkpoint-220/config.json\n",
      "Model weights saved in mar9/checkpoint-220/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-220/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-220/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-230\n",
      "Configuration saved in mar9/checkpoint-230/config.json\n",
      "Model weights saved in mar9/checkpoint-230/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-230/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-230/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-240\n",
      "Configuration saved in mar9/checkpoint-240/config.json\n",
      "Model weights saved in mar9/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-240/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-250\n",
      "Configuration saved in mar9/checkpoint-250/config.json\n",
      "Model weights saved in mar9/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-250/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-260\n",
      "Configuration saved in mar9/checkpoint-260/config.json\n",
      "Model weights saved in mar9/checkpoint-260/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-260/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-260/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-270\n",
      "Configuration saved in mar9/checkpoint-270/config.json\n",
      "Model weights saved in mar9/checkpoint-270/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-270/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-270/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-280\n",
      "Configuration saved in mar9/checkpoint-280/config.json\n",
      "Model weights saved in mar9/checkpoint-280/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-280/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-280/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-290\n",
      "Configuration saved in mar9/checkpoint-290/config.json\n",
      "Model weights saved in mar9/checkpoint-290/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-290/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-290/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-300\n",
      "Configuration saved in mar9/checkpoint-300/config.json\n",
      "Model weights saved in mar9/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-300/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-310\n",
      "Configuration saved in mar9/checkpoint-310/config.json\n",
      "Model weights saved in mar9/checkpoint-310/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-310/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-310/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-320\n",
      "Configuration saved in mar9/checkpoint-320/config.json\n",
      "Model weights saved in mar9/checkpoint-320/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-320/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-320/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-330\n",
      "Configuration saved in mar9/checkpoint-330/config.json\n",
      "Model weights saved in mar9/checkpoint-330/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-330/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-330/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-340\n",
      "Configuration saved in mar9/checkpoint-340/config.json\n",
      "Model weights saved in mar9/checkpoint-340/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-340/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-340/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-350\n",
      "Configuration saved in mar9/checkpoint-350/config.json\n",
      "Model weights saved in mar9/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-350/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-360\n",
      "Configuration saved in mar9/checkpoint-360/config.json\n",
      "Model weights saved in mar9/checkpoint-360/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-360/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-360/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-370\n",
      "Configuration saved in mar9/checkpoint-370/config.json\n",
      "Model weights saved in mar9/checkpoint-370/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-370/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-370/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-380\n",
      "Configuration saved in mar9/checkpoint-380/config.json\n",
      "Model weights saved in mar9/checkpoint-380/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-380/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-380/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-390\n",
      "Configuration saved in mar9/checkpoint-390/config.json\n",
      "Model weights saved in mar9/checkpoint-390/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-390/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-390/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-400\n",
      "Configuration saved in mar9/checkpoint-400/config.json\n",
      "Model weights saved in mar9/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-400/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-410\n",
      "Configuration saved in mar9/checkpoint-410/config.json\n",
      "Model weights saved in mar9/checkpoint-410/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-410/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-410/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-420\n",
      "Configuration saved in mar9/checkpoint-420/config.json\n",
      "Model weights saved in mar9/checkpoint-420/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-420/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-420/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-430\n",
      "Configuration saved in mar9/checkpoint-430/config.json\n",
      "Model weights saved in mar9/checkpoint-430/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-430/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-430/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-440\n",
      "Configuration saved in mar9/checkpoint-440/config.json\n",
      "Model weights saved in mar9/checkpoint-440/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-440/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-440/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-450\n",
      "Configuration saved in mar9/checkpoint-450/config.json\n",
      "Model weights saved in mar9/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-450/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-460\n",
      "Configuration saved in mar9/checkpoint-460/config.json\n",
      "Model weights saved in mar9/checkpoint-460/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-460/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-460/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-470\n",
      "Configuration saved in mar9/checkpoint-470/config.json\n",
      "Model weights saved in mar9/checkpoint-470/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-470/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-470/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-480\n",
      "Configuration saved in mar9/checkpoint-480/config.json\n",
      "Model weights saved in mar9/checkpoint-480/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-480/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-480/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-490\n",
      "Configuration saved in mar9/checkpoint-490/config.json\n",
      "Model weights saved in mar9/checkpoint-490/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-490/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-490/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-500\n",
      "Configuration saved in mar9/checkpoint-500/config.json\n",
      "Model weights saved in mar9/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-510\n",
      "Configuration saved in mar9/checkpoint-510/config.json\n",
      "Model weights saved in mar9/checkpoint-510/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-510/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-510/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-520\n",
      "Configuration saved in mar9/checkpoint-520/config.json\n",
      "Model weights saved in mar9/checkpoint-520/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-520/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-520/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-530\n",
      "Configuration saved in mar9/checkpoint-530/config.json\n",
      "Model weights saved in mar9/checkpoint-530/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-530/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-530/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-540\n",
      "Configuration saved in mar9/checkpoint-540/config.json\n",
      "Model weights saved in mar9/checkpoint-540/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-540/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-540/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-550\n",
      "Configuration saved in mar9/checkpoint-550/config.json\n",
      "Model weights saved in mar9/checkpoint-550/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-550/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-550/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-560\n",
      "Configuration saved in mar9/checkpoint-560/config.json\n",
      "Model weights saved in mar9/checkpoint-560/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-560/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-560/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-570\n",
      "Configuration saved in mar9/checkpoint-570/config.json\n",
      "Model weights saved in mar9/checkpoint-570/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-570/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-570/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-580\n",
      "Configuration saved in mar9/checkpoint-580/config.json\n",
      "Model weights saved in mar9/checkpoint-580/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-580/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-580/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-590\n",
      "Configuration saved in mar9/checkpoint-590/config.json\n",
      "Model weights saved in mar9/checkpoint-590/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-590/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-590/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-600\n",
      "Configuration saved in mar9/checkpoint-600/config.json\n",
      "Model weights saved in mar9/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-600/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-610\n",
      "Configuration saved in mar9/checkpoint-610/config.json\n",
      "Model weights saved in mar9/checkpoint-610/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-610/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-610/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-620\n",
      "Configuration saved in mar9/checkpoint-620/config.json\n",
      "Model weights saved in mar9/checkpoint-620/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-620/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-620/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-630\n",
      "Configuration saved in mar9/checkpoint-630/config.json\n",
      "Model weights saved in mar9/checkpoint-630/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-630/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-630/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-640\n",
      "Configuration saved in mar9/checkpoint-640/config.json\n",
      "Model weights saved in mar9/checkpoint-640/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-640/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-640/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-650\n",
      "Configuration saved in mar9/checkpoint-650/config.json\n",
      "Model weights saved in mar9/checkpoint-650/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-650/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-650/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-660\n",
      "Configuration saved in mar9/checkpoint-660/config.json\n",
      "Model weights saved in mar9/checkpoint-660/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-660/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-660/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 208\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to mar9/checkpoint-670\n",
      "Configuration saved in mar9/checkpoint-670/config.json\n",
      "Model weights saved in mar9/checkpoint-670/pytorch_model.bin\n",
      "tokenizer config file saved in mar9/checkpoint-670/tokenizer_config.json\n",
      "Special tokens file saved in mar9/checkpoint-670/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=678, training_loss=0.18508839112181755, metrics={'train_runtime': 4677.7374, 'train_samples_per_second': 2.309, 'train_steps_per_second': 0.145, 'total_flos': 1171049751864000.0, 'train_loss': 0.18508839112181755, 'epoch': 6.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Trainer parameters\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "# Define Trainer\n",
    "# args = TrainingArguments(\n",
    "#     output_dir='output',\n",
    "#     evaluation_strategy='steps',\n",
    "#     eval_steps=250,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=10,\n",
    "#     learning_rate=2e-5,\n",
    "#     seed=0,\n",
    "#     report_to='none'\n",
    "# )\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='mar9',\n",
    "#     evaluation_strategy='steps',\n",
    "#     eval_steps=25,\n",
    "#     save_strategy='steps',\n",
    "#     save_steps=25,\n",
    "#     logging_steps=25,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=6,\n",
    "#     learning_rate=2e-5,\n",
    "#     weight_decay=0.01,\n",
    "#     report_to='none'\n",
    "# )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='mar9',\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=10,\n",
    "    save_strategy='steps',\n",
    "    save_steps=10,\n",
    "    logging_steps=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=6,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77c34265-fcd6-4a09-bbd3-316a365844bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ----- 3. Predict -----#\n",
    "# # # Load test data\n",
    "# # test_data = df\n",
    "# # X_test = list(test_data['label'])\n",
    "# # X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# # Create torch dataset\n",
    "# test_dataset = Dataset(X_val_tokenized)\n",
    "\n",
    "# # Loading fine-tuned model\n",
    "# model_path = 'test-trainer/checkpoint-2000'\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "# test_trainer = Trainer(model)\n",
    "\n",
    "\n",
    "# raw_pred, _, _ = test_trainer.predict(test_dataset)\n",
    "# y_pred = np.argmax(raw_pred, axis=1)\n",
    "\n",
    "# y_pred = pd.Series(y_pred)\n",
    "# goal_indices = list(t[t==1].index)\n",
    "\n",
    "# c = 1\n",
    "# for i in goal_indices:\n",
    "#     print(f'Goal{c}')\n",
    "#     print(X_val[i])\n",
    "#     print('')\n",
    "#     c += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (AI kit)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
